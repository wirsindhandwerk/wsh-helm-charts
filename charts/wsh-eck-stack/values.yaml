eck-operator:
  enabled: true
  # the CRDs may already be installed in the cluster from other namespaces
  installCRDs: false
  # same as CRDs, these may already be available.
  createClusterScopedResources: false
  managedNamespaces: []
  # whether to enable the webhook.
  webhook:
    enabled: false
  config:
    validateStorageClass: false

# FIXME: for each entry in this config map, a user should be managed in the elasticsearch cluster.
users:
  # configure the elastic master user too
  # elastic-admin: elastic
  elastic-admin: "$2y$10$nu6Gy1yMjVXKuV7u6s1GcO4egqJZLHMtazMsyyj0O6uDB9vFu0/vq"
  # example-user: example-password
  example-user: "$2y$10$OrZF7nYE37TBmblHsKRTz.vsrzuI3PwpIzn94F5Hhl/gfW.CblhcG"
  # foo: bar
  foo: "$2y$10$XfUHfcdluEe6wAIFnd1NIu2vBGn7lZQ0x2NaSBi.hIB.pVwQVK8SC"

user_roles:
  admin: elastic-admin
  kibana: example-user,foo

    

# FIXME: configurable rollover for indexes. 
# can be done per index (the filebeat index is the important one here)
# -> roll over the index every 5GB so that automated rebalancing of data nodes works better,
# which could allow us to better reduce / increase the storage size according to current needs.
# -> purge / delete logs to free up space after x days, 14 should be the default.
indexLifecycle:
  rollover: 5GB
  deleteAfterDays: 14d # example

# the frontend is automatically integrated into the upstream nginx
kibana:
  frontendPrefix: /logs
  replicas: 1
  resources:
    requests:
      memory: 500m
      cpu: 0.25
    limits:
      memory: 1Gi
      cpu: 0.5

elasticsearch:
  master:
    replicas: 3
  data:
    replicas: 3
    # maximum amount of storage per data pod
    storage: 100Gi
    # configure your own storage class
    storageClassName: default